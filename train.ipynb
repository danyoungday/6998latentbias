{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c93b21-317b-458e-ad88-f24054d16b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3145aabd-3398-414f-b5e2-b2865991906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        o = self.linear2(h)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss\n",
    "    \n",
    "    def predict(self, pos, neg):\n",
    "        x0 = torch.tensor(self.normalize(pos), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(neg), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62cbae30-bb5b-4e46-8e47-99cae54e3dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 1600) (290, 1600) (290,)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"saved/\")\n",
    "TRIAL_NAME = \"first_try\"\n",
    "root = os.path.join(path, TRIAL_NAME)\n",
    "neg_hs = np.load(os.path.join(root, \"fem-hs.npy\"))\n",
    "pos_hs = np.load(os.path.join(root, \"male-hs.npy\"))\n",
    "y = np.load(os.path.join(root, \"y.npy\"))\n",
    "\n",
    "neg_hs, pos_hs = neg_hs[:,-1,:], pos_hs[:,-1,:]\n",
    "\n",
    "print(neg_hs.shape, pos_hs.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8439d9d9-44ee-4861-a64f-0b444d040773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 1600) (290, 1600) (290,)\n",
      "0.7327586206896551\n",
      "lr f1 score: 0.829268292682927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(y)\n",
    "p = np.random.permutation(n)\n",
    "neg_hs = neg_hs[p]\n",
    "pos_hs = pos_hs[p]\n",
    "y = y[p]\n",
    "\n",
    "print(neg_hs.shape, pos_hs.shape, y.shape)\n",
    "s = int(n*0.6)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:s], neg_hs[s:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:s], pos_hs[s:]\n",
    "y_train, y_test = y[:s], y[s:]\n",
    "print(y_test.sum()/len(y_test))\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "lr_pred = lr.predict(x_test)\n",
    "print(f\"lr f1 score: {f1_score(y_test, lr_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4709342d-129a-431e-b8e4-6f951d940480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs = CCS(neg_hs_train, pos_hs_train, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6756524a-5b8e-4d9d-ae57-8e4627da1c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccs loss: 0.15563657879829407\n"
     ]
    }
   ],
   "source": [
    "loss = ccs.repeated_train()\n",
    "print(f\"ccs loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c06756f2-d4e1-47e1-ad87-69e4f046e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label distribution: 0.7327586206896551\n",
      "f1 score: 0.7058823529411765\n",
      "accuracy score: 0.6120689655172413\n",
      "[[17 14]\n",
      " [31 54]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ccs.predict(neg_hs_test, pos_hs_test)\n",
    "print(f\"label distribution: {y_test.sum()/len(y_test)}\")\n",
    "print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a354971-0946-495a-b73c-466499e7ff52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ae41e-0546-43a5-b456-21b1c5c6fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
