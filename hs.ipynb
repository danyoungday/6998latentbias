{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e0d07b9-39c6-456c-a3ab-2c4e5f961770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c655bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSUS_GROUPS = [\"Women\", \"White\", \"Black or African American\", \"Asian\", \"Hispanic or Latino\"]\n",
    "\n",
    "SAVE_DIR = os.path.join(os.getcwd(), \"saved/\")\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "CACHE_DIR = os.path.join(os.getcwd(), \"cache_dir\")\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "PROFESSIONS_PATH = os.path.join(DATA_DIR, \"professions.json\")\n",
    "CENSUS_PATH = os.path.join(DATA_DIR, \"cpsaat11.csv\")\n",
    "PROMPTS_PATH = os.path.join(DATA_DIR, \"prompts.txt\")\n",
    "PROMPTS_PATH_CENSUS = os.path.join(DATA_DIR, \"census_race_prompts.txt\")\n",
    "CROWSPAIRS_PATH = os.path.join(DATA_DIR, \"crows_pairs_anonymized.csv\")\n",
    "\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca816fbe-b0e0-4a7b-bdee-02d9b2e3d680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff25434a-6c6c-4bbf-8347-49d4c986f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Get the CLS token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,0,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Still only get the last token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "        \n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"decoder_hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "     # If we do not specify a layer, get them all. Get the last hidden state in the decoder\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    return hs\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, model_type, tokenizer, neg_prompts, pos_prompts, layer):\n",
    "    \"\"\"\n",
    "    Returns N x L X D arrays of hidden states.\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for neg, pos in tqdm(zip(neg_prompts, pos_prompts)):\n",
    "        # get hidden states\n",
    "        if model_type == \"encoder\":\n",
    "            get_hidden_states = get_encoder_hidden_states\n",
    "        elif model_type == \"decoder\":\n",
    "            get_hidden_states = get_decoder_hidden_states\n",
    "        elif model_type == \"encoder-decoder\":\n",
    "            get_hidden_states = get_encoder_decoder_hidden_states\n",
    "        else:\n",
    "            assert False, \"Invalid model type\"\n",
    "\n",
    "        neg_hs = get_hidden_states(model, tokenizer, neg, layer=layer)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, pos, layer=layer)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "    \n",
    "    # Stack into single array\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "        \n",
    "    return all_neg_hs, all_pos_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db318cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_profession(prompt, text, label):\n",
    "    \"\"\"\n",
    "    Prompts contain a <LABEL0/LABEL1> tag and a <TEXT> tag.\n",
    "    Replace the label tag with the corresponding label, replace the text tag with the text.\n",
    "    \"\"\"\n",
    "    # First replace the <TEXT> tag with the proper text\n",
    "    output = re.sub(r'<TEXT>', text, prompt)\n",
    "\n",
    "    # Replace the <LABEL0/LABEL1> tag with the proper label\n",
    "    template = re.findall(r'<(.*?)>', output)\n",
    "    labels = template[0].split(\"/\")\n",
    "    output = re.sub(r'<(.*?)>', labels[label], output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_professions(professions_path, prompt, undersample=False):\n",
    "    \"\"\"\n",
    "    Reads professions.json and loads professions that have nonzero stereotypical male/female\n",
    "    bias scores. Parses professions into given prompt. Creates parallel label array as well.\n",
    "    :param undersample: Whether or not to take the top k most biased male professions to match\n",
    "    the k female professions.\n",
    "    \"\"\"\n",
    "    with open(professions_path, \"r\") as f:\n",
    "        professions = json.load(f)\n",
    "    \n",
    "    profs = np.array([prof[0].replace(\"_\", \" \") for prof in professions])\n",
    "    # reals = np.array([prof[1] for prof in professions])\n",
    "    biases = np.array([prof[2] for prof in professions])\n",
    "\n",
    "    fem_idx = np.where(biases < 0)[0]\n",
    "    male_idx = np.where(biases > 0)[0]\n",
    "    if undersample:\n",
    "        male_idx = np.flip(np.argsort(biases))[:fem_idx.shape[0]]\n",
    "    idx = np.concatenate([fem_idx, male_idx])\n",
    "    prof_sample = profs[idx]\n",
    "    labels = [0 for _ in range(len(fem_idx))] + [1 for _ in range(len(male_idx))]\n",
    "\n",
    "    # Parse into prompt\n",
    "    neg_prompts = [format_profession(prompt, prof, 0) for prof in prof_sample]\n",
    "    pos_prompts = [format_profession(prompt, prof, 1) for prof in prof_sample]\n",
    "\n",
    "    return neg_prompts, pos_prompts, labels\n",
    "\n",
    "\n",
    "def parse_census_data(census_path, prompts, by=\"Women\"):\n",
    "    census_df = pd.read_csv(census_path)\n",
    "\n",
    "    # Drop NaNs\n",
    "    census_df = census_df.replace('–', pd.NA) # This is a weird '-' character present in the data\n",
    "    census_df = census_df.dropna()\n",
    "\n",
    "    census_df[CENSUS_GROUPS] = census_df[CENSUS_GROUPS].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    sorted_df = None\n",
    "    if by == \"Black or African American\":\n",
    "        census_df[\"whitenorm\"] = (census_df[\"White\"] - census_df[\"White\"].mean()) / census_df[\"White\"].std()\n",
    "        census_df[\"blacknorm\"] = (census_df[\"Black or African American\"] - \\\n",
    "                                  census_df[\"Black or African American\"].mean()) / \\\n",
    "                                    census_df[\"Black or African American\"].std()\n",
    "\n",
    "        census_df[\"racediff\"] = census_df[\"whitenorm\"] - census_df[\"blacknorm\"]\n",
    "        sorted_df = census_df.sort_values(by=\"racediff\", ascending=False)\n",
    "        sorted_df = sorted_df[abs(sorted_df[\"racediff\"]) >= 1]\n",
    "        sorted_df[\"label\"] = sorted_df[\"racediff\"] > 1\n",
    "\n",
    "    elif by == \"Women\":\n",
    "        # Get most and least female dominated jobs\n",
    "        sorted_df = census_df.sort_values(by=by, ascending=False)\n",
    "        sorted_df = sorted_df[abs(sorted_df[by] - 0.5) > 0.1]\n",
    "        sorted_df[\"label\"] = sorted_df[by] < 0.5\n",
    "\n",
    "    else:\n",
    "        assert False, \"Give a valid group to sort by\"\n",
    "    \n",
    "    return np.array(sorted_df[\"Occupation\"].tolist()), np.array(sorted_df[\"label\"].tolist(), dtype=int)\n",
    "\n",
    "\n",
    "def parse_crowspairs(crowspairs_path):\n",
    "    df = pd.read_csv(crowspairs_path)\n",
    "    df = df[df[\"stereo_antistereo\"] == \"stereo\"]\n",
    "\n",
    "    # Randomly shuffle around which are positive and which are negative so\n",
    "    # we have even label distribution.\n",
    "    true = df[\"sent_more\"].to_numpy()\n",
    "    false = df[\"sent_less\"].to_numpy()\n",
    "    labels = np.random.randint(2, size=len(true))\n",
    "    neg_prompts = np.where(labels == 0, true, false)\n",
    "    pos_prompts = np.where(labels == 1, true, false)\n",
    "\n",
    "    return neg_prompts, pos_prompts, labels\n",
    "\n",
    "\n",
    "def parse_prompts():\n",
    "    with open(PROMPTS_PATH, \"r\") as f:\n",
    "        prompts = [prompt.strip(\"\\n\") for prompt in f.readlines()]\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c24333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden_states(model_name, model_type, trial_name, neg_prompts, pos_prompts, y, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in 3 parallel lists of pos and neg prompts as well as their label and passes them through\n",
    "    the given model, outputting 2 sets of hidden states for each layer.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    if model_type == \"encoder\":\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    if model_type == \"decoder\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    elif model_type == \"encoder-decoder\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Get hidden states\n",
    "    all_neg, all_pos = get_hidden_states_many_examples(model, model_type, tokenizer, neg_prompts, pos_prompts, layer=None)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if verbose:\n",
    "        print(all_neg.shape, all_pos.shape, y.shape)\n",
    "\n",
    "    # Save hidden states\n",
    "    root = os.path.join(SAVE_DIR, trial_name)\n",
    "    if not os.path.exists(root):\n",
    "        print(f\"Creating directory {root}\")\n",
    "        os.makedirs(root)\n",
    "    np.save(os.path.join(root, \"fem-hs.npy\"), all_neg)\n",
    "    np.save(os.path.join(root, \"male-hs.npy\"), all_pos)\n",
    "    np.save(os.path.join(root, \"y.npy\"), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32352f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_professions_trials(trials):\n",
    "    \"\"\"\n",
    "    Saves hidden states for each trial in saved/professions/trialname_promptX\n",
    "    \"\"\"\n",
    "    # Read in data\n",
    "    prompts = parse_prompts()\n",
    "\n",
    "    # Pass data throug hidden states\n",
    "    for trial in trials:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"Creating hs for {trial['model_type']} model {trial['model_name']} with prompt {prompt}\")\n",
    "            # Create prompts from professions\n",
    "            neg_prompts, pos_prompts, y = parse_professions(PROFESSIONS_PATH, prompt, undersample=False)\n",
    "\n",
    "            save_hidden_states(\n",
    "                model_name=trial[\"model_name\"], \n",
    "                model_type=trial[\"model_type\"],\n",
    "                trial_name=f\"professions/{trial['trial_name']}_prompt{i}\",\n",
    "                neg_prompts=neg_prompts,\n",
    "                pos_prompts=pos_prompts,\n",
    "                y=y, \n",
    "                verbose=True)\n",
    "            \n",
    "\n",
    "def save_crowspairs_trials(trials):\n",
    "    # Pass data throug hidden states\n",
    "    for trial in trials:\n",
    "        print(f\"Creating hs for {trial['model_type']} model {trial['model_name']} with crowspairs\")\n",
    "        # Create prompts from professions\n",
    "        neg_prompts, pos_prompts, y = parse_crowspairs(CROWSPAIRS_PATH)\n",
    "\n",
    "        save_hidden_states(\n",
    "            model_name=trial[\"model_name\"], \n",
    "            model_type=trial[\"model_type\"],\n",
    "            trial_name=f\"crowspairs/{trial['trial_name']}\",\n",
    "            neg_prompts=neg_prompts,\n",
    "            pos_prompts=pos_prompts,\n",
    "            y=y, \n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d76eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_trials = [\n",
    "    {\"trial_name\": \"gpt2\",\n",
    "     \"model_name\": \"gpt2\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-large\",\n",
    "     \"model_name\": \"gpt2-large\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-xl\",\n",
    "     \"model_name\": \"gpt2-xl\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-medium\",\n",
    "     \"model_name\": \"gpt2-medium\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "]\n",
    "\n",
    "roberta_trials = [\n",
    "    {\"trial_name\": \"roberta-base\",\n",
    "     \"model_name\": \"roberta-base\",\n",
    "     \"model_type\": \"encoder\"},\n",
    "     {\"trial_name\": \"roberta-large\",\n",
    "     \"model_name\": \"roberta-large\",\n",
    "     \"model_type\": \"encoder\"},\n",
    "]\n",
    "\n",
    "flan_t5_trials = [\n",
    "    {\"trial_name\": \"flan-t5-small\",\n",
    "     \"model_name\": \"google/flan-t5-small\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"flan-t5-base\",\n",
    "     \"model_name\": \"google/flan-t5-base\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"flan-t5-large\",\n",
    "     \"model_name\": \"google/flan-t5-large\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"flan-t5-xl\",\n",
    "     \"model_name\": \"google/flan-t5-xl\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33cd6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hs for encoder-decoder model google/flan-t5-small with crowspairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1290it [03:04,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 9, 512) (1290, 9, 512) (1290,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/flan-t5-small\n",
      "Creating hs for encoder-decoder model google/flan-t5-base with crowspairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bb9e247e8e459e81c5b8aaaa9e8616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ed4aea8624fb7ab565c21289dd35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffebd7476c6943aaa0bf38baff7d53cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a356827ccfe45d19c30a54fdf8637e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd8646a5b8148c4b39c5d3ebd88f7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f384a3babf014552b4d84a029048ff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1906008c7244c25ac85b158ab8d987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1290it [04:18,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 13, 768) (1290, 13, 768) (1290,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/flan-t5-base\n",
      "Creating hs for encoder-decoder model google/flan-t5-large with crowspairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [04:52,  3.43it/s]"
     ]
    }
   ],
   "source": [
    "save_crowspairs_trials(flan_t5_trials)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
