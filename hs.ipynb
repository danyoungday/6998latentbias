{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d07b9-39c6-456c-a3ab-2c4e5f961770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c655bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(os.getcwd(), \"saved/\")\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.mkdir(SAVE_DIR)\n",
    "CACHE_DIR = os.path.join(os.getcwd(), \"cache_dir\")\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca816fbe-b0e0-4a7b-bdee-02d9b2e3d680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25434a-6c6c-4bbf-8347-49d4c986f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Get the CLS token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,0,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Still only get the last token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "        \n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"encoder_hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def format_profession(prompt, text, label):\n",
    "    \"\"\"\n",
    "    Prompts contain a <LABEL0/LABEL1> tag and a <TEXT> tag.\n",
    "    Replace the label tag with the corresponding label, replace the text tag with the text.\n",
    "    \"\"\"\n",
    "    # First replace the <TEXT> tag with the proper text\n",
    "    output = re.sub(r'<TEXT>', text, prompt)\n",
    "\n",
    "    # Replace the <LABEL0/LABEL1> tag with the proper label\n",
    "    template = re.findall(r'<(.*?)>', output)\n",
    "    labels = template[0].split(\"/\")\n",
    "    output = re.sub(r'<(.*?)>', labels[label], output)\n",
    "    return output\n",
    "\n",
    "def get_hidden_states_many_examples(model, model_type, tokenizer, prompt, data, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for text in tqdm(data):\n",
    "        # get hidden states\n",
    "        if model_type == \"encoder\":\n",
    "            neg_hs = get_encoder_hidden_states(model, tokenizer, format_profession(prompt, text, 0), layer=layer)\n",
    "            pos_hs = get_encoder_hidden_states(model, tokenizer, format_profession(prompt, text, 1), layer=layer)\n",
    "        elif model_type == \"decoder\":\n",
    "            neg_hs = get_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 0), layer=layer)\n",
    "            pos_hs = get_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 1), layer=layer)\n",
    "        elif model_type == \"encoder-decoder\":\n",
    "            neg_hs = get_encoder_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 0), layer=layer)\n",
    "            pos_hs = get_encoder_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 1), layer=layer)\n",
    "        else:\n",
    "            assert False, \"Invalid model type\"\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "    \n",
    "    # Stack into single array\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "        \n",
    "    return all_neg_hs, all_pos_hs\n",
    "\n",
    "def parse_professions(professions_path, undersample=False):\n",
    "    with open(professions_path, \"r\") as f:\n",
    "        professions = json.load(f)\n",
    "    \n",
    "    profs = np.array([prof[0].replace(\"_\", \" \") for prof in professions])\n",
    "    # reals = np.array([prof[1] for prof in professions])\n",
    "    biases = np.array([prof[2] for prof in professions])\n",
    "\n",
    "    fem_idx = np.where(biases < 0)[0]\n",
    "    male_idx = np.where(biases > 0)[0]\n",
    "    if undersample:\n",
    "        male_idx = np.flip(np.argsort(biases))[:fem_idx.shape[0]]\n",
    "    idx = np.concatenate([fem_idx, male_idx])\n",
    "    labels = np.array([0 for _ in range(len(fem_idx))] + [1 for _ in range(len(male_idx))])\n",
    "\n",
    "    return profs[idx], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden_states(model_name, model_type, prompt, trial_name, professions, y, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Creating hidden states for the {model_type} model {model_name} using prompt {prompt}\")\n",
    "        \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    if model_type == \"encoder\":\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    if model_type == \"decoder\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    elif model_type == \"encoder-decoder\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Get hidden states\n",
    "    all_neg, all_pos = get_hidden_states_many_examples(model, model_type, tokenizer, prompt, professions, layer=None)\n",
    "\n",
    "    if verbose:\n",
    "        print(all_neg.shape, all_pos.shape, y.shape)\n",
    "\n",
    "    # Save hidden states\n",
    "    root = os.path.join(SAVE_DIR, trial_name)\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    np.save(os.path.join(root, \"fem-hs.npy\"), all_neg)\n",
    "    np.save(os.path.join(root, \"male-hs.npy\"), all_pos)\n",
    "    np.save(os.path.join(root, \"y.npy\"), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32352f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in prompts\n",
    "prompts = []\n",
    "with open(os.path.join(os.getcwd(), \"prompts.txt\"), \"r\") as f:\n",
    "    prompts = [prompt.strip(\"\\n\") for prompt in f.readlines()]\n",
    "trials = []\n",
    "trials += [\n",
    "    {\n",
    "        \"trial_name\": f\"flan-t5-large_undersample_prompt{i}\",\n",
    "        \"model_name\": \"google/flan-t5-large\",\n",
    "        \"model_type\": \"encoder-decoder\",\n",
    "        \"prompt\": i\n",
    "    }\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "professions, y = parse_professions(\"professions.json\", undersample=False)\n",
    "for trial in trials:\n",
    "    save_hidden_states(\n",
    "        model_name=trial[\"model_name\"], \n",
    "        model_type=trial[\"model_type\"],\n",
    "        prompt=prompts[trial[\"prompt\"]],\n",
    "        trial_name=trial[\"trial_name\"],\n",
    "        professions=professions,\n",
    "        y=y, \n",
    "        verbose=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
