{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0d07b9-39c6-456c-a3ab-2c4e5f961770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca816fbe-b0e0-4a7b-bdee-02d9b2e3d680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff25434a-6c6c-4bbf-8347-49d4c986f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Still only get the last token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "        \n",
    "    return hs\n",
    "\n",
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "def format_profession(text, label):\n",
    "    return f\"The following profession refers to a person who's gender is f{['female', 'male'][label]}:\\n{text}\"\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for text in tqdm(data):\n",
    "        # get hidden states\n",
    "        neg_hs = get_decoder_hidden_states(model, tokenizer, format_profession(text, 0), layer=layer)\n",
    "        pos_hs = get_decoder_hidden_states(model, tokenizer, format_profession(text, 1), layer=layer)\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "    \n",
    "    # Stack into single array\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "        \n",
    "    return all_neg_hs, all_pos_hs\n",
    "\n",
    "def parse_professions(professions_path):\n",
    "    with open(professions_path, \"r\") as f:\n",
    "        professions = json.load(f)\n",
    "    \n",
    "    profession_list = []\n",
    "    labels = []\n",
    "    for profession in professions:\n",
    "        if profession[2] < 0:\n",
    "            labels.append(0)\n",
    "            profession_list.append(profession[0])\n",
    "        elif profession[2] > 0:\n",
    "            labels.append(1)\n",
    "            profession_list.append(profession[0])\n",
    "    \n",
    "    return np.array(profession_list), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7395ebe-e391-45c0-957c-5645837937a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"\n",
    "\n",
    "cache_dir = \"cache_dir\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "model.cuda()\n",
    "professions, y = parse_professions(\"professions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bbb02e-98b8-4fcf-ba33-d9c7605c288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:29<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = os.path.join(os.getcwd(), \"saved/\")\n",
    "TRIAL_NAME = \"first_try\"\n",
    "root = os.path.join(SAVE_DIR, TRIAL_NAME)\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "all_neg, all_pos = get_hidden_states_many_examples(model, tokenizer, professions, layer=None)\n",
    "print(all_neg.shape, all_pos.shape, y.shape)\n",
    "np.save(os.path.join(root, \"fem-hs.npy\"), all_neg)\n",
    "np.save(os.path.join(root, \"male-hs.npy\"), all_pos)\n",
    "np.save(os.path.join(root, \"y.npy\"), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ccee6-dd58-4b03-a425-1f8163f8fa98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4719c336-819e-4575-9ce8-0a0c36290081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize (adding the EOS token this time)\n",
    "input_ids = tokenizer(\"0 1 2 3 4 5\" + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8491a5d1-ee00-46bd-a9ff-b1c7e999a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 1600])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concatenate(output[\"hidden_states\"], dim=0)[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176046b5-a708-4fa4-aa00-5dadd6c8f4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
