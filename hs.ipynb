{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7e0d07b9-39c6-456c-a3ab-2c4e5f961770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0c655bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSUS_GROUPS = [\"Women\", \"White\", \"Black or African American\", \"Asian\", \"Hispanic or Latino\"]\n",
    "\n",
    "SAVE_DIR = os.path.join(os.getcwd(), \"saved/\")\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "CACHE_DIR = os.path.join(os.getcwd(), \"cache_dir\")\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "PROFESSIONS_PATH = os.path.join(DATA_DIR, \"professions.json\")\n",
    "CENSUS_PATH = os.path.join(DATA_DIR, \"cpsaat11.csv\")\n",
    "PROMPTS_PATH = os.path.join(DATA_DIR, \"prompts.txt\")\n",
    "PROMPTS_PATH_CENSUS = os.path.join(DATA_DIR, \"census_race_prompts.txt\")\n",
    "CROWSPAIRS_PATH = os.path.join(DATA_DIR, \"crows_pairs_anonymized.csv\")\n",
    "\n",
    "SPLIT = 32\n",
    "\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca816fbe-b0e0-4a7b-bdee-02d9b2e3d680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ff25434a-6c6c-4bbf-8347-49d4c986f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inputs(input_list, batch_size):\n",
    "    return [input_list[i:i+batch_size] for i in range(0, len(input_list), batch_size)]\n",
    "\n",
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer=None):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Get the CLS token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,0,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_hidden_states_tokens(model, tokenizer, input_text_list):\n",
    "    input_text_list = input_text_list.tolist()\n",
    "    max_len = max([len(tokenizer.encode(text)) for text in input_text_list])\n",
    "    n = len(input_text_list)\n",
    "    all_hs = []\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        split_text_list = input_text_list[i : min(i + (n // SPLIT), n)]\n",
    "        i += (n // SPLIT)\n",
    "        input = tokenizer(split_text_list, padding=\"max_length\", max_length=max_len, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input, output_hidden_states=True)\n",
    "        hs_layers = outputs[\"hidden_states\"]\n",
    "        hs = hs_layers[-1]\n",
    "        all_hs.append(hs)\n",
    "\n",
    "    return torch.concatenate(all_hs, dim=0)\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=None):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Still only get the last token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "        \n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states_tokens(model, tokenizer, input_text_list):\n",
    "    input_text_list = [text + tokenizer.eos_token for text in input_text_list]\n",
    "    max_len = max([len(tokenizer.encode(text)) for text in input_text_list])\n",
    "    n = len(input_text_list)\n",
    "    all_hs = []\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        split_text_list = input_text_list[i : min(i + (n // SPLIT), n)]\n",
    "        i += (n // SPLIT)\n",
    "        input = tokenizer(split_text_list, padding=\"max_length\", max_length=max_len, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input, output_hidden_states=True)\n",
    "        hs_layers = outputs[\"hidden_states\"]\n",
    "        hs = hs_layers[-1]\n",
    "        all_hs.append(hs)\n",
    "    \n",
    "    return torch.concatenate(all_hs, dim=0)\n",
    "\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    BATCH_SIZE = 16\n",
    "    batches = batch_inputs(input_text.tolist(), BATCH_SIZE)\n",
    "    hs_list = []\n",
    "    for batch in tqdm(batches):\n",
    "        tokens = tokenizer(batch, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "        # The pad token is prepended to t5 output\n",
    "        decoder_text_ids = tokenizer([tokenizer.pad_token for _ in range(len(batch))], return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model(**tokens, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "        # get the appropriate hidden states\n",
    "        hs_tuple = output[\"encoder_hidden_states\"]\n",
    "        hs_batch = torch.stack(hs_tuple, dim=1)[:, :, -1, :].squeeze()\n",
    "        hs_list.append(hs_batch)\n",
    "    \n",
    "    hs = torch.concatenate(hs_list, dim=0)\n",
    "    hs = hs.detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states_tokens(model, tokenizer, input_text_list):\n",
    "    input_text_list = input_text_list.tolist()\n",
    "    max_len = max([len(tokenizer.encode(text)) for text in input_text_list])\n",
    "    n = len(input_text_list)\n",
    "    all_hs = []\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        split_text_list = input_text_list[i : min(i + (n // SPLIT), n)]\n",
    "        decoder_text_list = [\"\" for _ in range(len(split_text_list))]\n",
    "        i += (n // SPLIT)\n",
    "        input = tokenizer(split_text_list, padding=\"max_length\", max_length=max_len, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        decoder_input = tokenizer(decoder_text_list, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input, decoder_input_ids=decoder_input, output_hidden_states=True)\n",
    "        hs_enc = outputs[\"encoder_hidden_states\"][-1]\n",
    "        hs_dec = outputs[\"decoder_hidden_states\"][-1]\n",
    "        hs = torch.concatenate((hs_enc, hs_dec), dim=1)\n",
    "        all_hs.append(hs)\n",
    "    \n",
    "    return torch.concatenate(all_hs, dim=0)\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, model_type, tokenizer, neg_prompts, pos_prompts, layer):\n",
    "    \"\"\"\n",
    "    Returns N x L X D arrays of hidden states.\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "\n",
    "    if layer:\n",
    "        if model_type == \"encoder\":\n",
    "                get_hidden_states = get_encoder_hidden_states\n",
    "        elif model_type == \"decoder\":\n",
    "            get_hidden_states = get_decoder_hidden_states\n",
    "        elif model_type == \"encoder-decoder\":\n",
    "            get_hidden_states = get_encoder_decoder_hidden_states\n",
    "        else:\n",
    "            assert False, \"Invalid model type\"\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        if model_type == \"encoder\":\n",
    "                get_hidden_states = get_encoder_hidden_states_tokens\n",
    "        elif model_type == \"decoder\":\n",
    "            get_hidden_states = get_decoder_hidden_states_tokens\n",
    "        elif model_type == \"encoder-decoder\":\n",
    "            get_hidden_states = get_encoder_decoder_hidden_states_tokens\n",
    "        else:\n",
    "            assert False, \"Invalid model type\"\n",
    "\n",
    "    neg_hs = get_hidden_states(model, tokenizer, neg_prompts)\n",
    "    pos_hs = get_hidden_states(model, tokenizer, pos_prompts)\n",
    "\n",
    "    return neg_hs, pos_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "db318cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_profession(prompt, text, label):\n",
    "    \"\"\"\n",
    "    Prompts contain a <LABEL0/LABEL1> tag and a <TEXT> tag.\n",
    "    Replace the label tag with the corresponding label, replace the text tag with the text.\n",
    "    \"\"\"\n",
    "    # First replace the <TEXT> tag with the proper text\n",
    "    output = re.sub(r'<TEXT>', text, prompt)\n",
    "\n",
    "    # Replace the <LABEL0/LABEL1> tag with the proper label\n",
    "    template = re.findall(r'<(.*?)>', output)\n",
    "    labels = template[0].split(\"/\")\n",
    "    output = re.sub(r'<(.*?)>', labels[label], output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_professions(professions_path, prompt, undersample=False):\n",
    "    \"\"\"\n",
    "    Reads professions.json and loads professions that have nonzero stereotypical male/female\n",
    "    bias scores. Parses professions into given prompt. Creates parallel label array as well.\n",
    "    :param undersample: Whether or not to take the top k most biased male professions to match\n",
    "    the k female professions.\n",
    "    \"\"\"\n",
    "    with open(professions_path, \"r\") as f:\n",
    "        professions = json.load(f)\n",
    "    \n",
    "    profs = np.array([prof[0].replace(\"_\", \" \") for prof in professions])\n",
    "    # reals = np.array([prof[1] for prof in professions])\n",
    "    biases = np.array([prof[2] for prof in professions])\n",
    "\n",
    "    fem_idx = np.where(biases < 0)[0]\n",
    "    male_idx = np.where(biases > 0)[0]\n",
    "    if undersample:\n",
    "        male_idx = np.flip(np.argsort(biases))[:fem_idx.shape[0]]\n",
    "    idx = np.concatenate([fem_idx, male_idx])\n",
    "    prof_sample = profs[idx]\n",
    "    labels = [0 for _ in range(len(fem_idx))] + [1 for _ in range(len(male_idx))]\n",
    "\n",
    "    # Parse into prompt\n",
    "    neg_prompts = [format_profession(prompt, prof, 0) for prof in prof_sample]\n",
    "    pos_prompts = [format_profession(prompt, prof, 1) for prof in prof_sample]\n",
    "\n",
    "    return neg_prompts, pos_prompts, labels\n",
    "\n",
    "\n",
    "def parse_census_data(census_path, prompts, by=\"Women\"):\n",
    "    census_df = pd.read_csv(census_path)\n",
    "\n",
    "    # Drop NaNs\n",
    "    census_df = census_df.replace('–', pd.NA) # This is a weird '-' character present in the data\n",
    "    census_df = census_df.dropna()\n",
    "\n",
    "    census_df[CENSUS_GROUPS] = census_df[CENSUS_GROUPS].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    sorted_df = None\n",
    "    if by == \"Black or African American\":\n",
    "        census_df[\"whitenorm\"] = (census_df[\"White\"] - census_df[\"White\"].mean()) / census_df[\"White\"].std()\n",
    "        census_df[\"blacknorm\"] = (census_df[\"Black or African American\"] - \\\n",
    "                                  census_df[\"Black or African American\"].mean()) / \\\n",
    "                                    census_df[\"Black or African American\"].std()\n",
    "\n",
    "        census_df[\"racediff\"] = census_df[\"whitenorm\"] - census_df[\"blacknorm\"]\n",
    "        sorted_df = census_df.sort_values(by=\"racediff\", ascending=False)\n",
    "        sorted_df = sorted_df[abs(sorted_df[\"racediff\"]) >= 1]\n",
    "        sorted_df[\"label\"] = sorted_df[\"racediff\"] > 1\n",
    "\n",
    "    elif by == \"Women\":\n",
    "        # Get most and least female dominated jobs\n",
    "        sorted_df = census_df.sort_values(by=by, ascending=False)\n",
    "        sorted_df = sorted_df[abs(sorted_df[by] - 0.5) > 0.1]\n",
    "        sorted_df[\"label\"] = sorted_df[by] < 0.5\n",
    "\n",
    "    else:\n",
    "        assert False, \"Give a valid group to sort by\"\n",
    "    \n",
    "    return np.array(sorted_df[\"Occupation\"].tolist()), np.array(sorted_df[\"label\"].tolist(), dtype=int)\n",
    "\n",
    "\n",
    "def parse_crowspairs(crowspairs_path, filter=None):\n",
    "    df = pd.read_csv(crowspairs_path)\n",
    "    df = df[df[\"stereo_antistereo\"] == \"stereo\"]\n",
    "\n",
    "    # Filter by filter\n",
    "    if filter:\n",
    "        df = df[df[\"bias_type\"].isin(filter)]\n",
    "\n",
    "    # Randomly shuffle around which are positive and which are negative so\n",
    "    # we have even label distribution.\n",
    "    more = df[\"sent_more\"].to_numpy()\n",
    "    less = df[\"sent_less\"].to_numpy()\n",
    "    labels = np.random.randint(2, size=len(more), )\n",
    "    neg_prompts = np.where(labels == 0, more, less)\n",
    "    pos_prompts = np.where(labels == 1, more, less)\n",
    "\n",
    "    return neg_prompts, pos_prompts, labels\n",
    "    \n",
    "\n",
    "def parse_prompts():\n",
    "    with open(PROMPTS_PATH, \"r\") as f:\n",
    "        prompts = [prompt.strip(\"\\n\") for prompt in f.readlines()]\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c24333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden_states(model_name, model_type, trial_name, neg_prompts, pos_prompts, y, layer, verbose=False):\n",
    "    \"\"\"\n",
    "    Takes in 3 parallel lists of pos and neg prompts as well as their label and passes them through\n",
    "    the given model, outputting 2 sets of hidden states for each layer.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR, model_max_length=512)\n",
    "    if model_type == \"encoder\":\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    if model_type == \"decoder\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif model_type == \"encoder-decoder\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Get hidden states\n",
    "    neg_hs, pos_hs = get_hidden_states_many_examples(model, model_type, tokenizer, neg_prompts, pos_prompts, layer)\n",
    "\n",
    "    if verbose:\n",
    "        print(neg_hs.shape, pos_hs.shape, y.shape)\n",
    "\n",
    "    # Save hidden states\n",
    "    root = os.path.join(SAVE_DIR, trial_name)\n",
    "    if not os.path.exists(root):\n",
    "        print(f\"Creating directory {root}\")\n",
    "        os.makedirs(root)\n",
    "    \n",
    "    np.save(os.path.join(root, \"fem-hs.npy\"), neg_hs)\n",
    "    np.save(os.path.join(root, \"male-hs.npy\"), pos_hs)\n",
    "    np.save(os.path.join(root, \"y.npy\"), np.array(y))\n",
    "\n",
    "    # if not layer and not os.path.exists(os.path.join(root, \"neg-lens.pt\")):\n",
    "    #     neg_lens = torch.tensor([len(tokenizer.encode(text)) for text in neg_prompts])\n",
    "    #     pos_lens = torch.tensor([len(tokenizer.encode(text)) for text in pos_prompts])\n",
    "    #     # Because we added an EOS token\n",
    "    #     if model_type == \"decoder\":\n",
    "    #         neg_lens += 1\n",
    "    #         pos_lens += 1\n",
    "    #     torch.save(neg_lens, os.path.join(root, \"neg-lens.pt\"))\n",
    "    #     torch.save(pos_lens, os.path.join(root, \"pos-lens.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "32352f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_professions_trials(trials):\n",
    "    \"\"\"\n",
    "    Saves hidden states for each trial in saved/professions/trialname_promptX\n",
    "    \"\"\"\n",
    "    # Read in data\n",
    "    prompts = parse_prompts()\n",
    "\n",
    "    # Pass data throug hidden states\n",
    "    for trial in trials:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"Creating hs for {trial['model_type']} model {trial['model_name']} with prompt {prompt}\")\n",
    "            # Create prompts from professions\n",
    "            neg_prompts, pos_prompts, y = parse_professions(PROFESSIONS_PATH, prompt, undersample=False)\n",
    "\n",
    "            save_hidden_states(\n",
    "                model_name=trial[\"model_name\"], \n",
    "                model_type=trial[\"model_type\"],\n",
    "                trial_name=f\"professions/{trial['trial_name']}_prompt{i}\",\n",
    "                neg_prompts=neg_prompts,\n",
    "                pos_prompts=pos_prompts,\n",
    "                y=y, \n",
    "                layer=True,\n",
    "                verbose=True)\n",
    "            \n",
    "\n",
    "def save_crowspairs_trials(trials, layer, filter=None, force=False):\n",
    "    # Pass data through hidden states\n",
    "    for trial in trials:\n",
    "        if filter[0] == \"all\" and len(filter) == 1:\n",
    "            filter = None\n",
    "        # For save path\n",
    "        prefix = \"crowspairs/\" if layer else \"crowspairs-token/\"\n",
    "        if filter:\n",
    "            filter = sorted(filter)\n",
    "            prefix += \"_\".join(filter) + \"/\"\n",
    "        full_trial_name = f\"{prefix}{trial['trial_name']}\"\n",
    "\n",
    "        # If we already have something saved here, skip it\n",
    "        if os.path.exists(os.path.join(SAVE_DIR, full_trial_name)) and not force:\n",
    "            print(f\"Already exists hs for {trial['model_type']} model {trial['model_name']} with crowspairs {filter} \")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Creating hs for {trial['model_type']} model {trial['model_name']} with crowspairs {filter} across {'layers' if layer else 'tokens'}\")\n",
    "            # Create prompts from professions\n",
    "            neg_prompts, pos_prompts, y = parse_crowspairs(CROWSPAIRS_PATH, filter=filter)\n",
    "            save_hidden_states(\n",
    "                model_name=trial[\"model_name\"], \n",
    "                model_type=trial[\"model_type\"],\n",
    "                trial_name=full_trial_name,\n",
    "                neg_prompts=neg_prompts,\n",
    "                pos_prompts=pos_prompts,\n",
    "                y=y, \n",
    "                layer=layer,\n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1d76eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_trials = [\n",
    "    {\"trial_name\": \"gpt2\",\n",
    "     \"model_name\": \"gpt2\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-large\",\n",
    "     \"model_name\": \"gpt2-large\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-xl\",\n",
    "     \"model_name\": \"gpt2-xl\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "     {\"trial_name\": \"gpt2-medium\",\n",
    "     \"model_name\": \"gpt2-medium\",\n",
    "     \"model_type\": \"decoder\"},\n",
    "]\n",
    "\n",
    "roberta_trials = [\n",
    "    {\"trial_name\": \"roberta-base\",\n",
    "     \"model_name\": \"roberta-base\",\n",
    "     \"model_type\": \"encoder\"},\n",
    "     {\"trial_name\": \"roberta-large\",\n",
    "     \"model_name\": \"roberta-large\",\n",
    "     \"model_type\": \"encoder\"},\n",
    "]\n",
    "\n",
    "flan_t5_trials = [\n",
    "    {\"trial_name\": \"flan-t5-small-test\",\n",
    "     \"model_name\": \"google/flan-t5-small\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"flan-t5-base\",\n",
    "     \"model_name\": \"google/flan-t5-base\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"flan-t5-large\",\n",
    "     \"model_name\": \"google/flan-t5-large\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "]\n",
    "\n",
    "t5_trials = [\n",
    "    {\"trial_name\": \"t5-small-enc\",\n",
    "     \"model_name\": \"t5-small\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"t5-base-enc\",\n",
    "     \"model_name\": \"t5-base\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "     {\"trial_name\": \"t5-large-enc\",\n",
    "     \"model_name\": \"t5-large\",\n",
    "     \"model_type\": \"encoder-decoder\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "33cd6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias types: ['race-color', 'socioeconomic', 'gender', 'disability', 'nationality', 'sexual-orientation', 'physical-appearance', 'religion', 'age']\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['race-color'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.16it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 7, 512) (473, 7, 512) (473,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/race-color/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['race-color'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.96it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 13, 768) (473, 13, 768) (473,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/race-color/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['race-color'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:52<00:00,  3.76s/it]\n",
      "100%|██████████| 30/30 [00:54<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(473, 25, 1024) (473, 25, 1024) (473,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/race-color/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['socioeconomic'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 7, 512) (157, 7, 512) (157,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/socioeconomic/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['socioeconomic'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 13, 768) (157, 13, 768) (157,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/socioeconomic/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['socioeconomic'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:44<00:00, 10.48s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 25, 1024) (157, 25, 1024) (157,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/socioeconomic/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['gender'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 7, 512) (159, 7, 512) (159,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/gender/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['gender'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.31it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 13, 768) (159, 13, 768) (159,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/gender/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['gender'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 25, 1024) (159, 25, 1024) (159,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/gender/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['disability'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 7, 512) (57, 7, 512) (57,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/disability/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['disability'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.01it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 13, 768) (57, 13, 768) (57,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/disability/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['disability'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.82s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 25, 1024) (57, 25, 1024) (57,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/disability/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['nationality'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 7, 512) (148, 7, 512) (148,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/nationality/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['nationality'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.58it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 13, 768) (148, 13, 768) (148,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/nationality/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['nationality'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:26<00:00,  8.66s/it]\n",
      "100%|██████████| 10/10 [00:23<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 25, 1024) (148, 25, 1024) (148,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/nationality/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['sexual-orientation'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.41it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7, 512) (72, 7, 512) (72,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/sexual-orientation/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['sexual-orientation'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.78it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 13, 768) (72, 13, 768) (72,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/sexual-orientation/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['sexual-orientation'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:24<00:00, 16.96s/it]\n",
      "100%|██████████| 5/5 [00:43<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 25, 1024) (72, 25, 1024) (72,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/sexual-orientation/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['physical-appearance'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 7, 512) (52, 7, 512) (52,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/physical-appearance/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['physical-appearance'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.05it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 13, 768) (52, 13, 768) (52,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/physical-appearance/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['physical-appearance'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 25, 1024) (52, 25, 1024) (52,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/physical-appearance/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['religion'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 7, 512) (99, 7, 512) (99,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/religion/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['religion'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.05it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 13, 768) (99, 13, 768) (99,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/religion/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['religion'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:41<00:00,  5.86s/it]\n",
      "100%|██████████| 7/7 [00:21<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 25, 1024) (99, 25, 1024) (99,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/religion/t5-large-enc\n",
      "Creating hs for encoder-decoder model t5-small with crowspairs ['age'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.90it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 7, 512) (73, 7, 512) (73,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/age/t5-small-enc\n",
      "Creating hs for encoder-decoder model t5-base with crowspairs ['age'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.46it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 13, 768) (73, 13, 768) (73,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/age/t5-base-enc\n",
      "Creating hs for encoder-decoder model t5-large with crowspairs ['age'] across layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.73s/it]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 25, 1024) (73, 25, 1024) (73,)\n",
      "Creating directory /Users/danyoung/workspace/columbia/sumgen/6998latentbias/saved/crowspairs/age/t5-large-enc\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CROWSPAIRS_PATH)\n",
    "filters = sorted(list(df[\"bias_type\"].unique()))\n",
    "print(f\"bias types: {filters}\")\n",
    "for filter in filters:\n",
    "    #save_crowspairs_trials(roberta_trials, False, [filter])\n",
    "    #save_crowspairs_trials(gpt2_trials, False, [filter])\n",
    "    save_crowspairs_trials(t5_trials, True, [filter], force=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
