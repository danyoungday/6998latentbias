{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0d07b9-39c6-456c-a3ab-2c4e5f961770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c655bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(os.getcwd(), \"saved/\")\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.mkdir(SAVE_DIR)\n",
    "CACHE_DIR = \"cache_dir\"\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca816fbe-b0e0-4a7b-bdee-02d9b2e3d680",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff25434a-6c6c-4bbf-8347-49d4c986f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Get the CLS token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,0,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    if layer:\n",
    "        hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    # If we do not specify a layer, get them all. Still only get the last token\n",
    "    else:\n",
    "        hs = torch.concatenate(hs_tuple, axis=0)[:,-1,:]\n",
    "        hs = hs.detach().cpu().numpy()\n",
    "        \n",
    "    return hs\n",
    "\n",
    "def format_profession(prompt, text, label):\n",
    "    \"\"\"\n",
    "    Prompts contain a <LABEL0/LABEL1> tag and a <TEXT> tag.\n",
    "    Replace the label tag with the corresponding label, replace the text tag with the text.\n",
    "    \"\"\"\n",
    "    # First replace the <TEXT> tag with the proper text\n",
    "    output = re.sub(r'<TEXT>', text, prompt)\n",
    "\n",
    "    # Replace the <LABEL0/LABEL1> tag with the proper label\n",
    "    template = re.findall(r'<(.*?)>', output)\n",
    "    labels = template[0].split(\"/\")\n",
    "    output = re.sub(r'<(.*?)>', labels[label], output)\n",
    "    return output\n",
    "\n",
    "def get_hidden_states_many_examples(model, model_type, tokenizer, prompt, data, layer):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop\n",
    "    for text in tqdm(data):\n",
    "        # get hidden states\n",
    "        if model_type == \"encoder\":\n",
    "            neg_hs = get_encoder_hidden_states(model, tokenizer, format_profession(prompt, text, 0), layer=layer)\n",
    "            pos_hs = get_encoder_hidden_states(model, tokenizer, format_profession(prompt, text, 1), layer=layer)\n",
    "        elif model_type == \"decoder\":\n",
    "            neg_hs = get_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 0), layer=layer)\n",
    "            pos_hs = get_decoder_hidden_states(model, tokenizer, format_profession(prompt, text, 1), layer=layer)\n",
    "        else:\n",
    "            assert False, \"Invalid model type\"\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "    \n",
    "    # Stack into single array\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "        \n",
    "    return all_neg_hs, all_pos_hs\n",
    "\n",
    "def parse_professions(professions_path):\n",
    "    with open(professions_path, \"r\") as f:\n",
    "        professions = json.load(f)\n",
    "    \n",
    "    profession_list = []\n",
    "    labels = []\n",
    "    for profession in professions:\n",
    "        if profession[2] < 0:\n",
    "            labels.append(0)\n",
    "            profession_list.append(profession[0])\n",
    "        elif profession[2] > 0:\n",
    "            labels.append(1)\n",
    "            profession_list.append(profession[0])\n",
    "    \n",
    "    return np.array(profession_list), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24333ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden_states(model_name, model_type, prompt, trial_name, professions, y, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Creating hidden states for the {model_type} model {model_name} using prompt {prompt}\")\n",
    "        \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Get hidden states\n",
    "    all_neg, all_pos = get_hidden_states_many_examples(model, model_type, tokenizer, prompt, professions, layer=None)\n",
    "\n",
    "    if verbose:\n",
    "        print(all_neg.shape, all_pos.shape, y.shape)\n",
    "\n",
    "    # Save hidden states\n",
    "    root = os.path.join(SAVE_DIR, trial_name)\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    np.save(os.path.join(root, \"fem-hs.npy\"), all_neg)\n",
    "    np.save(os.path.join(root, \"male-hs.npy\"), all_pos)\n",
    "    np.save(os.path.join(root, \"y.npy\"), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32352f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hidden states for the decoder model gpt2-large using prompt The following profession refers to a person who's gender is <female/male>: <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:44<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 37, 1280) (290, 37, 1280) (290,)\n",
      "Creating hidden states for the decoder model gpt2-large using prompt Only a <woman/man> can be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:48<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 37, 1280) (290, 37, 1280) (290,)\n",
      "Creating hidden states for the decoder model gpt2-large using prompt The <TEXT> is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:40<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 37, 1280) (290, 37, 1280) (290,)\n",
      "Creating hidden states for the decoder model gpt2-large using prompt When someone refers to a <TEXT>, I assume their gender is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 37, 1280) (290, 37, 1280) (290,)\n",
      "Creating hidden states for the decoder model gpt2-large using prompt A <man/woman> can't be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:46<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 37, 1280) (290, 37, 1280) (290,)\n",
      "Creating hidden states for the decoder model gpt2-medium using prompt The following profession refers to a person who's gender is <female/male>: <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:07<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the decoder model gpt2-medium using prompt Only a <woman/man> can be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:57<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the decoder model gpt2-medium using prompt The <TEXT> is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:04<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the decoder model gpt2-medium using prompt When someone refers to a <TEXT>, I assume their gender is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:03<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the decoder model gpt2-medium using prompt A <man/woman> can't be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:44<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the decoder model gpt2-xl using prompt The following profession refers to a person who's gender is <female/male>: <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:19<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n",
      "Creating hidden states for the decoder model gpt2-xl using prompt Only a <woman/man> can be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:17<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n",
      "Creating hidden states for the decoder model gpt2-xl using prompt The <TEXT> is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:03<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n",
      "Creating hidden states for the decoder model gpt2-xl using prompt When someone refers to a <TEXT>, I assume their gender is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:25<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n",
      "Creating hidden states for the decoder model gpt2-xl using prompt A <man/woman> can't be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:26<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 49, 1600) (290, 49, 1600) (290,)\n",
      "Creating hidden states for the encoder model roberta-large using prompt The following profession refers to a person who's gender is <female/male>: <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 290/290 [00:31<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the encoder model roberta-large using prompt Only a <woman/man> can be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 290/290 [00:27<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the encoder model roberta-large using prompt The <TEXT> is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 290/290 [00:23<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the encoder model roberta-large using prompt When someone refers to a <TEXT>, I assume their gender is <female/male>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 290/290 [00:28<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n",
      "Creating hidden states for the encoder model roberta-large using prompt A <man/woman> can't be a <TEXT>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 290/290 [00:26<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 25, 1024) (290, 25, 1024) (290,)\n"
     ]
    }
   ],
   "source": [
    "# Read in prompts\n",
    "prompts = []\n",
    "with open(os.path.join(os.getcwd(), \"prompts.txt\"), \"r\") as f:\n",
    "    prompts = [prompt.strip(\"\\n\") for prompt in f.readlines()]\n",
    "trials = []\n",
    "trials += [\n",
    "    {\n",
    "        \"trial_name\": f\"gpt2-large_prompt{i}\",\n",
    "        \"model_name\": \"gpt2-large\",\n",
    "        \"model_type\": \"decoder\",\n",
    "        \"prompt\": i\n",
    "    }\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "trials += [\n",
    "    {\n",
    "        \"trial_name\": f\"gpt2-medium_prompt{i}\",\n",
    "        \"model_name\": \"gpt2-medium\",\n",
    "        \"model_type\": \"decoder\",\n",
    "        \"prompt\": i\n",
    "    }\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "trials += [\n",
    "    {\n",
    "        \"trial_name\": f\"gpt2-xl_prompt{i}\",\n",
    "        \"model_name\": \"gpt2-xl\",\n",
    "        \"model_type\": \"decoder\",\n",
    "        \"prompt\": i\n",
    "    }\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "trials += [\n",
    "    {\n",
    "        \"trial_name\": f\"roberta-large_prompt{i}\",\n",
    "        \"model_name\": \"roberta-large\",\n",
    "        \"model_type\": \"encoder\",\n",
    "        \"prompt\": i\n",
    "    }\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "\n",
    "professions, y = parse_professions(\"professions.json\")\n",
    "for trial in trials:\n",
    "    save_hidden_states(\n",
    "        model_name=trial[\"model_name\"], \n",
    "        model_type=trial[\"model_type\"],\n",
    "        prompt=prompts[trial[\"prompt\"]],\n",
    "        trial_name=trial[\"trial_name\"],\n",
    "        professions=professions,\n",
    "        y=y, \n",
    "        verbose=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
